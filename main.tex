\documentclass[a4paper,12pt]{article}

\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{color}
\usepackage{listings}

\graphicspath{{figures/}}
\geometry{a4paper,left=2cm,right=2cm,top=1cm,bottom=1.8cm}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=cyan,
}

%opening
\title{acwj}
\author{Trganda}

\begin{document}

\maketitle

\section{Part 0: Introducation}

I've decided to go on a compiler writing journey.
In the past I've written some \href{https://github.com/DoctorWkt/pdp7-unix/blob/master/tools/as7}{assemblers}, 
and I've written a \href{https://github.com/DoctorWkt/h-compiler}{simaple compiler} for a typeless language.
But I've never written a compiler that can compile itself.
So that's where I'm headed on this journey.

As part of the process, I'm going to write up my work so that others can follow along.
This will also help me to clarify my thoughts and ideas. Hopefully you, and I, will find this useful!

\subsection{Goals of the journey}

Here are my goals, and non-goals, for the journey:

\begin{itemize}
    \item To write a self-compiling compiler. I think that if the compiler can compile itself, it gets to call itself a real compiler.
    \item To target at least one real hardware platform. I've seen a few compilers that generate code for hypothetical machines. 
          I want my compiler to work on real hardware. Also, if possible, I want to write the compiler so that it can support multiple backends for different hardware platforms.
    \item Pratical before research. There's a whole lot of research in the area of compilers. I want to start from absolute zero on this journey, so I'll tend to go for a practical approach and not a theory-heavy approach. That said, there will be times when I'll need to introduce (and implement) some theory-based stuff.
    \item Follow the KISS principle: keep it simple, stupid! I'm definitely going to be using Ken Thompson's principe here: "When in doubt, use brute force."
    \item Take a lot of small steps to reach the final goal. I'll break the journey up into a lot of simple steps instead of taking large leaps. This will make each new addition to the compiler a bite-sized and easily digestible thing.
\end{itemize}

\subsection{Target Language}

The choice of a target language is diffcult. If I choose a high-level language like \textbf{Python}, \textbf{Go} etc., then I'll have to implement a whole pile of libraries and classes as they are built-in to the language.

I could write a compiler for a language like Lisp, but these can be \href{ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-039.pdf}{done easily}.

Instead, I've fallen back on the old standby and I'm going to write a compiler for a subset of C, enough to allow the compiler to compile itself. C is just a step up from assembly language (for some subset of C, not \href{https://en.wikipedia.org/wiki/C18_(C_standard_revision)}{C18}), and this will help make the task of compiling the C code down to assembly somewhat easier. Oh, and I also like C.

\subsection{The Basic of a Compiler's Job}

The job of a compiler is to translate input in one language (usually a high-level language) into a different output language (usually a lower-level language than the input). The main steps are:

\begin{figure}[!ht]
    \centering
    \includegraphics[width= 0.9\textwidth]{parsing_steps.png}
\end{figure}

\begin{itemize}
    \item Do \href{https://en.wikipedia.org/wiki/Lexical_analysis}{lexical analysis} to recognise the lexical elements. In several languages, {\color{red}=} is different to {\color{red}==}, so you can't just read a single {\color{red}=}. We call these lexical elements \textit{tokens}.
    \item \href{https://en.wikipedia.org/wiki/Parsing}{Parse} the input, i.e. recognise the syntax and structural elements of the input and ensure that they conform to the \textit{grammar} of the language. For example, your language might have this decision-making structure:
    \begin{lstlisting}
if (x < 23) {
    print("x is smaller than 23\n");
}
    \end{lstlisting}
    but in another language you might write:
    \begin{lstlisting}
if (x < 23)
    print("x is smaller than 23\n");
    \end{lstlisting}
    This is also the place where the compiler cna detect syntax errors, like if the semicolon was missing on the end of first \textit{print} statement.
    \item Do \href{https://en.wikipedia.org/wiki/Semantic_analysis_(compilers)}{semantic analysis} of the input, i.e. understand the meaning of the input. This is actually different from recognising the syntax and structure. For example, in English, a sentence might have the form \textbf{<subject> <verb> <adjective> <object>}.
    \begin{lstlisting}
David ate lovely bananas.
Jennifer hates green tomatoes.
    \end{lstlisting}
    \item \href{https://en.wikipedia.org/wiki/Code_generation_(compiler)}{Translate} the meaning of the input into a different language. Here we convert the input, parts at a time, into a lower-level language.
\end{itemize}

\subsection{Resources}

There's a lot of comipler resources out on the Internet. Here are the ones I'll be looking at.

\subsubsection{Learning Resources}

If you want to start with some books, papers and tools on compilers, I'd highly recommend this list:

\begin{itemize}
    \item \href{https://github.com/aalhour/awesome-compilers}{Curated list of awesome resources on Compilers, Interpreters and Runtimes} by Ahmad Alhour
\end{itemize}

\subsubsection{Existing Compilers}

While I'm going to build my own compiler, I plan on looking at other compilers for ideas and probably also borrow some of their code. Here are the ones I'm looking at:

\begin{itemize}
    \item \href{http://www.t3x.org/subc/}{SubC} by Nils M Holm
    \item \href{https://github.com/rswier/swieros/blob/master/root/bin/c.c}{Swieros C Compiler} by Robert Swierczek
    \item \href{https://github.com/DoctorWkt/fbcc}{fbcc} by Fabrice Bellard
    \item \href{https://bellard.org/tcc}{tcc}, also by Fabrice Bellard and others
    \item \href{https://github.com/yui0/catc}{catc}  by Yuichiro Nakada
    \item \href{https://github.com/jserv/amacc}{amacc} by Jim Huang
    \item \href{https://en.wikipedia.org/wiki/Small-C}{Small C} by Ron Cain, James E. Hendrix, derivatives by others
\end{itemize}

In particular, I'll be using a lot of the ideas, and some of the code, from the SubC compiler.

\subsection{Setting Up the Development Enviroment}

Assuming that you want to come along on this journey, here's what you'll need. I'm going to use a Linux development enviroment, so download and set up your favourite Linux system: I'm using Lubuntu $18.04$.

I'm going to target two hardware platforms: Intel x86-64 and 32-bit ARM. I'll use a PC running Lubuntu $18.04$ as the Intel target, and a Raspberry Pi running Raspbian as the ARM target.

On the Intel platform, we are going to need an existing C compiler. So, install this package.
If there are any more tools required for a vanilla Linux system let me know.

Finally, clone a copy of this Github repository.

\subsection{The Next Step}

In the next part of our compiler writing journey, we will start with the code to scan our input file and find the \textit{tokens} that are the lexical elements of our language.

\section{Part 1: Introduction to Lexical Scanning}

We start our compiler writing journey with a simple lexical scanner. As I mentioned in the previous part, the job of the scanner is to identify the lexical elements, or \textit{tokens}, in the input language.

We will start with a language that has only five lexical elements:
\begin{itemize}
    \item the four basic maths operators: {+}, {/}, {+} and {-}
    \item decimal whole numbers which have 1 or more digits 0 .. 9
\end{itemize}

Each token that we scan is going to be stored in this structure (from \textbf{defs.h})

\begin{lstlisting}
// Token structure
struct token {
    int token;
    int intvalue;
};
\end{lstlisting}

where the \textbf{token} field can be one of these values (from \textbf{defs.h}):

\begin{lstlisting}
// Tokens
enum {
    T_PLUS, T_MINUS, T_STAR, T_SLASH, T_INTLIT
};
\end{lstlisting}

When the token is a $T\_INTLIT$ (i.e. an integer literal), the $intvalue$ field will hold the value of the integer that we scanned in.

\subsection{Functions in $scan.c$}

The $scan.c$ file holds the functions of our lexical scanner. We are going to read in one character at a time from our input file. However, there will be times when we need to "put back" a character if we have read too far ahead in the input stream. We also want to track what line we are currently on so that we can print the line number in our debug messages. All of this is done by the $next()$ function:

\begin{lstlisting}
static int next(void) {
    int c;
  
    if (Putback) {                // Use the character put
        c = Putback;              // back if there is one
        Putback = 0;
        return c;
    }
        
    c = fgetc(Infile);            // Read from input file
    if ('\n' == c)
    Line++;                       // Increment line count
    return c;
}
\end{lstlisting}

The $Putback$ and $Line$ variables are defined in $data.h$ along with our input file pointer:

\begin{lstlisting}
extern_ int     Line;
extern_ int     Putback;
extern_ FILE    *Infile;
\end{lstlisting}

All C files will include this where $extern\_$ is replaced with $extern$. But $main.c$ will remove the $extern\_$; hence, these variables will "belong" to $main.c$.

Finally, how do we put a character back into the input stream? Thus:

\begin{lstlisting}
// Put back an unwanted character
static void putback(int c) {
    Putback = c;
}
\end{lstlisting}

\subsection{Ignoring Whitespace}

We need a function that reads and silently skips whitespace characters until it gets a non-whitesapce character, and returns it. Thus:

\begin{lstlisting}
// Skip past input that we don't nee   d to deal with,
// i.e. whitesapce, newlines. Return the first
// character we do need to deal with.
static int skip(void) {
    int c;

    c = next();
    while (' ' == c || '\t' == c || '\n' == c || '\r' == c || '\f' == c) {
        c = next();
    }
    return (c);
}
\end{lstlisting}

\subsection{Scanning Tokens: $scan()$}

So new we can read characters in while skipping whitesapce; we can also put back a character if we read one character too far ahead. We can now write our first lexical scanner:

\begin{lstlisting}
    // Scan and return the next token found in the input.
    // Return 1 if token valid, 0 if no tokens left.
    int scan(struct token *t) {
      int c;
    
      // Skip whitespace
      c = skip();
    
      // Determine the token based on
      // the input character
      switch (c) {
      case EOF:
        return (0);
      case '+':
        t->token = T_PLUS;
        break;
      case '-':
        t->token = T_MINUS;
        break;
      case '*':
        t->token = T_STAR;
        break;
      case '/':
        t->token = T_SLASH;
        break;
      default:
        // More here soon
      }
    
      // We found a token
      return (1);
    }    
\end{lstlisting}

That's it for the simple one-character tokens: for each recognised character, turn it into a token. You may ask: why not just put the recognised character into the $struct token$? The answer is that later we will need to recognised multi-character tokens such as $==$ and keywords like $if$ and $while$. So it will make life easier to have an enumerated list of token values.

\subsection{Integer Literal Values}

In fact, we already have to face this situation as we also need to recoginse integer literal values like $3827$ and $87731$. Here is the missing $default$ code from the $switch$ statement:

\begin{lstlisting}
default:
    // If it's a digit, scan the
    // literal integer value in
    if (isdigit(c)) {
      t->intvalue = scanint(c);
      t->token = T_INTLIT;
      break;
    }

    printf("Unrecognised character %c on line %d\n", c, Line);
    exit(1);
\end{lstlisting}

Once we hit a decimal digit character, we call the helper function $scanint()$ with this first character. It will return the scanned integer value. To do this, it has to read each character in turn, check that it's a legitimate digit, and build up the final number. Here is the code:

\begin{lstlisting}
// Scan and return an integer literal
// value from the input file. Store
// the value as a string in Text.
static int scanint(int c) {
    int k, val = 0;
    
    // Convert each character into an int value
    while ((k = chrpos("0123456789", c)) >= 0) {
        val = val * 10 + k;
        c = next();
    }
    
    // We hit a non-integer character, put it back.
    putback(c);
    return val;
}
\end{lstlisting}

We start with a zero $val$ value. Each time we get a character in the set $0$ to $9$, we covert this to an $int$ value with $chrpos()$. We make $val$ 10 times bigger and then add this new digit to it.

For example, if we have the character $3$, $2$, $8$, we do:

\begin{itemize}
    \item $val = 0 * 10 + 3$, i.e. 3
    \item $val = 3 * 10 + 2$, i.e. 32
    \item $val = 32 * 10 + 8$, i.e. 328
\end{itemize}

Right at the end, did you notice the call to $putback(c)$? We found a character that's not a decimal digit at this point. We can't simply discard it, but luckily we can put it back in the input stream to be consumed later.

You may also ask at this point: why not simply substract the ASCII value of $0$ from $c$ to make it an integer? The answer is that, later on, we will be able to do $chrpos("0123456789abcdef")$ to convert hexadecimal digits as well.

Here's the code for $chrpos()$:

\begin{lstlisting}
    // Return the position of character c
    // in string s, or -1 if c not found
    static int chrpos(char *s, int c) {
      char *p;
    
      p = strchr(s, c);
      return (p ? p - s : -1);
    }
\end{lstlisting}

And that's it for the lexical scanner code in $scan.c$ for now.

\subsection{Putting the Scanner to Work}

The code in $main.c$ puts the above scanner to work. The $main()$ function opens up a file and then scans it for tokens:

\begin{lstlisting}
    void main(int argc, char *argv[]) {
        ...
        init();
        ...
        Infile = fopen(argv[1], "r");
        ...
        scanfile();
        exit(0);
    }
\end{lstlisting}

And $scanfile()$ loops while there is new token and prints out the details of the token:

\begin{lstlisting}
    // List of printable tokens
    char *tokstr[] = { "+", "-", "*", "/", "intlit" };
    
    // Loop scanning in all the tokens in the input file.
    // Print out details of each token found.
    static void scanfile() {
        struct token T;
    
        while (scan(&T)) {
            printf("Token %s", tokstr[T.token]);
            if (T.token == T_INTLIT)
                printf(", value %d", T.intvalue);
            printf("\n");
        }
    }
\end{lstlisting}

\subsection{Some Example Input Files}

I've provided some example input files so you can see what tokens the scanner finds in each file, and what input files the scanner rejects.

\begin{lstlisting}
    $ make
    cc -o scanner -g main.c scan.c
    
    $ cat input01
    2 + 3 * 5 - 8 / 3
    
    $ ./scanner input01
    Token intlit, value 2
    Token +
    Token intlit, value 3
    Token *
    Token intlit, value 5
    Token -
    Token intlit, value 8
    Token /
    Token intlit, value 3
    
    $ cat input04
    23 +
    18 -
    45.6 * 2
    / 18
    
    $ ./scanner input04
    Token intlit, value 23
    Token +
    Token intlit, value 18
    Token -
    Token intlit, value 45
    Unrecognised character . on line 3
\end{lstlisting}

\subsection{Conclusion and What's Next}

We've started small and we have a simple lexical scanner that recognises the four main maths operators and also integer values. We saw that we needed to skip whitespace and put back characters if we read too far into the input.

Single character tokens are easy to scan, but multi-character tokens are a bit harder. But at the end, the $scan()$ function returns the next token from the input file in a $struct token$ variable.

In the next part of our compiler writing journey, we will build a recursive descent parser to interpret the grammer of our input files, and calculate \& print out the final value for each file.

\section{Part 2: Introduction to Parsing}

In this part of our compiler writing journey, I'm going to introduce the basics of a parser. As I mentioned in the first part, the job of the parser is to recognise the syntax and structural elements of the input and ensure that they conform to the $grammer$ of the language.

We already have several language elements that we can scan in, i.e. our tokens:

\begin{itemize}
    \item the four basic maths operators: $*, /, +$ and $-$
    \item deciaml whole numbers which have $1$ or more digits $0$ .. $9$
\end{itemize}

Now let's define a grammer for the language that our parser will recognise.

\subsection{BNF: Backus-Naur Form}

You will come across the use of \href{https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form}{BNF} at some point if you get into dealing with computer languages. I will just introduce enough of the BNF syntax here to express the grammar we want to recognise.

We want a grammer to express maths expressions with whole numbers. Here is the BNF description of the grammer:

\begin{lstlisting}
    expression: number
    | expression '*' expression
    | expression '/' expression
    | expression '+' expression
    | expression '-' expression
    ;

    number:  T_INTLIT
    ;
\end{lstlisting}

The vertical bars separate options in the grammar, so the above says:

\begin{itemize}
    \item An expression could be just a number, or
    \item An expression is two expressions separated by a $*$ token, or
    \item An expression is two expressions separated by a $/$ token, or
    \item An expression is two expressions separated by a $+$ token, or
    \item An expression is two expressions separated by a $-$ token, or
    \item A number is always a $T\_INTLIT$ token
\end{itemize}

\end{document}
